**1. Create a Python script to generate graphs for four different activation functions:**

![Figure_1](https://github.com/M23CSA013/Activity_1/assets/142054900/f3c19464-1623-4a68-bd25-58b3c5bc8de9)

**2. Create three branches: "feature-1", "feature-2", and "bug-fix".**
   
  &emsp;**i) feature-1 branch:**

  &emsp;Random Values: [-3.50, -1.20, 0.00, 2.80, -4.10, 1.50, -0.70, 3.20, -2.40, 4.60]
  
  &emsp;Sigmoid Values: [0.0293, 0.2315, 0.5000, 0.9427, 0.0163, 0.8176, 0.3318, 0.9608, 0.0832, 0.9900]

  &emsp;**ii) feature-2 branch:**

  &emsp;ReLU Values: [0., 0., 0., 2.8, 0., 1.5, 0., 3.2, 0., 4.6]

  &emsp;Leaky ReLU Values: [-0.035, -0.012, 0., 2.8, -0.041, 1.5, -0.007, 3.2, -0.024, 4.6]

  &emsp;Tanh Values: [-0.9981779, -0.83365461, 0., 0.99263152, -0.99945084, 0.90514825, -0.60436778, 0.9966824, -0.98367486, 0.99979794]

  &emsp;**iii) bug-fix branch:**

  &emsp;Corrected the ReLU implementation to handle negative values properly.

